{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions and get results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/del/anaconda3/envs/allennmt/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from allennlp.models.archival import archive_model\n",
    "import pickle\n",
    "import numpy as np\n",
    "from allennlp.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#archive_model(\"models/tanh_2l_ff.json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.service.predictors import Predictor\n",
    "\n",
    "# required so that our custom model + predictor + dataset reader\n",
    "# will be registered by name\n",
    "import bigram_embedder\n",
    "\n",
    "# with open(\"data/step5_holduot_splits/train_all_shuffled.pkl\", 'rb') as f:\n",
    "#     train_zip = pickle.load(f)\n",
    "# with open(\"data/step5_holduot_splits/dev_all_shuffled.pkl\", 'rb') as f:\n",
    "#     dev_zip = pickle.load(f)\n",
    "with open(\"data/step5_holduot_splits/test_all_shuffled.pkl\", 'rb') as f:\n",
    "    test_zip = pickle.load(f)\n",
    "    \n",
    "\n",
    "#archive = load_archive('models/multi_tanh_300/model.tar.gz')\n",
    "#archive = load_archive('models/tanh_2l/model.tar.gz')\n",
    "archive = load_archive('models/4l_100/model.tar.gz')\n",
    "predictor = Predictor.from_archive(archive, 'bigram-embedder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/step5_holduot_splits/test_all_shuffled.tsv\") as f:\n",
    "    X_test = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_types_test = []\n",
    "bigram_vecs_test = []\n",
    "for l in X_test:\n",
    "    bt, bv, _, _ = l.split('\\t')\n",
    "    bigram_types_test.append(bt)\n",
    "    bigram_vecs_test.append([float(v) for v in bv.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_method = \"model\"\n",
    "#prediction_method = \"sum\"\n",
    "#prediction_method = \"minus\"\n",
    "#prediction_method = \"mul\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from operator import sub\n",
    "from operator import mul\n",
    "# summing up / avaraging\n",
    "\n",
    "if prediction_method == \"sum\":\n",
    "    bigram_vecs_test_hat = []\n",
    "    for ex in X_test:\n",
    "        ex = predictor.load_line(ex)\n",
    "        w1_vec = [float(i) for i in ex['w1_vec_str'].split()]\n",
    "        w2_vec = [float(i) for i in ex['w2_vec_str'].split()]\n",
    "        res_vec = list(map(add, w1_vec, w2_vec))\n",
    "        bigram_vecs_test_hat.append(res_vec)\n",
    "        #bigram_vecs_test_hat.append(predictor.predict_json(ex).get(\"bigram_vecs_hat\"))\n",
    "        # OR model\n",
    "elif prediction_method == \"minus\":\n",
    "    bigram_vecs_test_hat = []\n",
    "    for ex in X_test:\n",
    "        ex = predictor.load_line(ex)\n",
    "        w1_vec = [float(i) for i in ex['w1_vec_str'].split()]\n",
    "        w2_vec = [float(i) for i in ex['w2_vec_str'].split()]\n",
    "        res_vec = list(map(sub, w1_vec, w2_vec))\n",
    "        bigram_vecs_test_hat.append(res_vec)\n",
    "        #bigram_vecs_test_hat.append(predictor.predict_json(ex).get(\"bigram_vecs_hat\"))\n",
    "        # OR model\n",
    "elif prediction_method == \"mul\":\n",
    "    bigram_vecs_test_hat = []\n",
    "    for ex in X_test:\n",
    "        ex = predictor.load_line(ex)\n",
    "        w1_vec = [float(i) for i in ex['w1_vec_str'].split()]\n",
    "        w2_vec = [float(i) for i in ex['w2_vec_str'].split()]\n",
    "        res_vec = list(map(mul, w1_vec, w2_vec))\n",
    "        bigram_vecs_test_hat.append(res_vec)\n",
    "        #bigram_vecs_test_hat.append(predictor.predict_json(ex).get(\"bigram_vecs_hat\"))\n",
    "        # OR model\n",
    "elif prediction_method == \"model\":\n",
    "    bigram_vecs_test_hat = []\n",
    "    for ex in X_test:\n",
    "        ex = predictor.load_line(ex)\n",
    "        bigram_vecs_test_hat.append(predictor.predict_json(ex).get(\"bigram_vecs_hat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_smooth_l1_error(y, y_hat):\n",
    "    y_var = torch.autograd.Variable(torch.FloatTensor(y))\n",
    "    y_hat_var = torch.autograd.Variable(torch.FloatTensor(y_hat))\n",
    "    \n",
    "    if y_var.size() != y_hat_var.size():\n",
    "        smooth_l1_error = torch.nn.SmoothL1Loss(reduce=False)\n",
    "        y_var = y_var.expand(y_hat_var.size())\n",
    "        return smooth_l1_error.forward(y_var, y_hat_var).sum(1).data\n",
    "\n",
    "    else:\n",
    "        smooth_l1_error = torch.nn.SmoothL1Loss()\n",
    "        return smooth_l1_error.forward(y_var, y_hat_var).data[0]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comput accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(target, vecs, n):\n",
    "    sim_matrix = compute_smooth_l1_error(target, vecs)\n",
    "    nvecs, nids = sim_matrix.sort(0)\n",
    "    return nvecs[0:n], nids[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars\n",
    "PATH_DATA_STEP3_FOLDER = \"data/step3_fasttext_vectors/\"\n",
    "PATH_FASTTEXT_VECTORS = PATH_DATA_STEP3_FOLDER + \"vectors_likelihood_ratio-100-0.05.vec\"\n",
    "SUFFIX = PATH_FASTTEXT_VECTORS.split('vectors_')[1][0:-4]\n",
    "PATH_DATA_OUTPUT_FOLDER = \"data/step4_wordpairs_bigrams_vec_data/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_ngram_vecs():\n",
    "    # load fasttext .vec file containing unigrams and bigrams\n",
    "    with open(PATH_FASTTEXT_VECTORS, 'r') as f:\n",
    "        vectors_all = f.readlines()\n",
    "        \n",
    "    num_vectors_all, dim = vectors_all[0].split()\n",
    "    num_vectors_all, dim = int(num_vectors_all), int(dim)\n",
    "    \n",
    "    print(num_vectors_all)\n",
    "    print(dim)\n",
    "    \n",
    "    del vectors_all[0]\n",
    "    \n",
    "    ngram_types_all = []\n",
    "    ngram_vectors_all = []\n",
    "    for l in vectors_all:\n",
    "        l = l.split()\n",
    "        ngram_types_all.append(l[0])\n",
    "        ngram_vectors_all.append([float(v) for v in l[1:]])\n",
    "    return ngram_types_all, ngram_vectors_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808968\n",
      "100\n",
      "CPU times: user 29.8 s, sys: 1.43 s, total: 31.3 s\n",
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#all_bigrams, all_vecs, _ = zip(*train_zip + dev_zip + test_zip)\n",
    "all_bigrams, all_vecs = get_all_ngram_vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUM_NEIGHBOURS = 100\n",
    "nearest_vecs_test = []\n",
    "nearest_ids_test = []\n",
    "for i, vec_hat in enumerate(bigram_vecs_test_hat):\n",
    "    nvecs, nids = get_closest(vec_hat, all_vecs, NUM_NEIGHBOURS)\n",
    "    nearest_vecs_test.append(nvecs)\n",
    "    nearest_ids_test.append(nids)\n",
    "    if i % 100 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_bigrams = []\n",
    "for i, ids in enumerate(nearest_ids_test):\n",
    "    predicted_bigram_list = [all_bigrams[i] for i in ids]\n",
    "    predicted_bigrams.append(predicted_bigram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuaracy on test set: 70.88 %\n",
      "Top 3 accuaracy on test set: 84.76 %\n",
      "Top 5 accuaracy on test set: 87.76 %\n",
      "Top 7 accuaracy on test set: 90.0 %\n",
      "Top 10 accuaracy on test set: 91.72 %\n",
      "Top 100 accuaracy on test set: 97.0 %\n"
     ]
    }
   ],
   "source": [
    "Ns = [1,3,5,7,10, 100]\n",
    "for N in Ns:\n",
    "    corrects = []\n",
    "    corrects_groundtruth = []\n",
    "    errors = []\n",
    "    errors_groundtruth = [] \n",
    "    for i, predicted_bigrams_list in enumerate(predicted_bigrams):\n",
    "        if bigram_types_test[i] in predicted_bigrams_list[0:N]:\n",
    "            corrects.append(predicted_bigrams_list)\n",
    "            corrects_groundtruth.append(bigram_types_test[i])\n",
    "        else:\n",
    "            errors.append(predicted_bigrams_list)\n",
    "            errors_groundtruth.append(bigram_types_test[i])\n",
    "\n",
    "    print('Top', N, 'accuaracy on test set:', len(corrects) / len(predicted_bigrams) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuaracy on test set: 62.03999999999999 %\n",
      "Top 3 accuaracy on test set: 74.96000000000001 %\n",
      "Top 5 accuaracy on test set: 79.75999999999999 %\n",
      "Top 7 accuaracy on test set: 82.16 %\n",
      "Top 10 accuaracy on test set: 84.68 %\n",
      "Top 100 accuaracy on test set: 93.12 %\n"
     ]
    }
   ],
   "source": [
    "Ns = [1,3,5,7,10, 100]\n",
    "for N in Ns:\n",
    "    corrects = []\n",
    "    corrects_groundtruth = []\n",
    "    errors = []\n",
    "    errors_groundtruth = [] \n",
    "    for i, predicted_bigrams_list in enumerate(predicted_bigrams):\n",
    "        if bigram_types_test[i] in predicted_bigrams_list[0:N]:\n",
    "            corrects.append(predicted_bigrams_list)\n",
    "            corrects_groundtruth.append(bigram_types_test[i])\n",
    "        else:\n",
    "            errors.append(predicted_bigrams_list)\n",
    "            errors_groundtruth.append(bigram_types_test[i])\n",
    "\n",
    "    print('Top', N, 'accuaracy on test set:', len(corrects) / len(predicted_bigrams) * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 | of_rf -> ['statvolt', 'sincgars', 'qmv', 'currentfib', 'milliamperes']\n",
      "152 | ecw_s -> ['wwe_and', 'and_wcw', 'comeback_the', 'thefutureembrace', 'the_smackdown']\n",
      "169 | v_has -> ['however_has', 'is_likewise', 'and_furthermore', 'called_has', 'furthermore_since']\n",
      "84 | s_perhaps -> ['perhaps', 'perhaps_ironically', 'perhaps_surprisingly', 'perhaps_just', 'however_perhaps']\n",
      "74 | be_laid -> ['be_abandoned', 'laid_for', 'laid_and', 'being_laid', 'be_cleared']\n",
      "7 | as_pm -> ['pm', 'bqe', 'frequently_one', 'frequently_observed', 'as_show']\n",
      "165 | her_play -> ['playing_herself', 'mother_played', 'on_herself', 'janet_and', 'a_backstage']\n",
      "58 | met_in -> ['met', 'finally_met', 'attended_with', 'recalled_from', 'also_in']\n",
      "132 | iv_v -> ['iv', 'iii', 'v', 'vii', 'iii_this']\n",
      "82 | lb_or -> ['extra_weight', 'additional_weight', 'normal_weight', 'weight_five', 'zero_kcal']\n"
     ]
    }
   ],
   "source": [
    "random_n_ids = [random.randrange(0, len(errors)) for i in range(n)]\n",
    "\n",
    "for ind in random_n_ids:\n",
    "    ngrams = errors[ind]\n",
    "    print(ind, '|', errors_groundtruth[ind], '->', ngrams[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                  Type                          Data/Info\n",
      "-----------------------------------------------------------------\n",
      "Model                     type                          <class 'allennlp.models.model.Model'>\n",
      "N                         int                           100\n",
      "NUM_NEIGHBOURS            int                           100\n",
      "Ns                        list                          n=6\n",
      "PATH_DATA_OUTPUT_FOLDER   str                           data/step4_wordpairs_bigrams_vec_data/\n",
      "PATH_DATA_STEP3_FOLDER    str                           data/step3_fasttext_vectors/\n",
      "PATH_FASTTEXT_VECTORS     str                           data/step3_fasttext_vecto<...>lihood_ratio-100-0.05.vec\n",
      "Predictor                 type                          <class 'allennlp.service.<...>ors.predictor.Predictor'>\n",
      "SUFFIX                    str                           likelihood_ratio-100-0.05\n",
      "X_test                    list                          n=2500\n",
      "add                       builtin_function_or_method    <built-in function add>\n",
      "all_bigrams               list                          n=808968\n",
      "all_vecs                  list                          n=808968\n",
      "archive                   Archive                       Archive(model=BigramEmbed<...>bject at 0x7f79fef2b278>)\n",
      "archive_model             function                      <function archive_model at 0x7f7a04f15bf8>\n",
      "bigram_embedder           module                        <module 'bigram_embedder'<...>am_embedder/__init__.py'>\n",
      "bigram_types_test         list                          n=2500\n",
      "bigram_vecs_test          list                          n=2500\n",
      "bigram_vecs_test_hat      list                          n=2500\n",
      "bt                        str                           harbor_with\n",
      "bv                        str                           0.17536 0.32362 0.19353 0<...> -0.35614 -0.35311 1.2104\n",
      "compute_smooth_l1_error   function                      <function compute_smooth_<...>_error at 0x7f79fef278c8>\n",
      "corrects                  list                          n=2328\n",
      "corrects_groundtruth      list                          n=2328\n",
      "errors                    list                          n=172\n",
      "errors_groundtruth        list                          n=172\n",
      "ex                        dict                          n=2\n",
      "f                         TextIOWrapper                 <_io.TextIOWrapper name='<...>ode='r' encoding='UTF-8'>\n",
      "get_all_ngram_vecs        function                      <function get_all_ngram_vecs at 0x7f79fef40488>\n",
      "get_closest               function                      <function get_closest at 0x7f79fef276a8>\n",
      "i                         int                           2499\n",
      "ids                       LongTensor                    \\n 4.2218e+05\\n 6.1000e+0<...>LongTensor of size 100]\\n\n",
      "ind                       int                           82\n",
      "l                         str                           harbor_with\t0.17536 0.323<...>.35691 -0.0053568 0.35854\n",
      "load_archive              function                      <function load_archive at 0x7f7a04f15c80>\n",
      "mul                       builtin_function_or_method    <built-in function mul>\n",
      "n                         int                           10\n",
      "nearest_ids_test          list                          n=2500\n",
      "nearest_vecs_test         list                          n=2500\n",
      "ngrams                    list                          n=100\n",
      "nids                      LongTensor                    \\n 4.2218e+05\\n 6.1000e+0<...>LongTensor of size 100]\\n\n",
      "np                        module                        <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
      "nvecs                     FloatTensor                   \\n 1.0284\\n 1.2452\\n 1.46<...>loatTensor of size 100]\\n\n",
      "pickle                    module                        <module 'pickle' from '/h<...>lib/python3.6/pickle.py'>\n",
      "predicted_bigram_list     list                          n=100\n",
      "predicted_bigrams         list                          n=2500\n",
      "predicted_bigrams_list    list                          n=100\n",
      "prediction_method         str                           model\n",
      "predictor                 BigramEmbedderPredictor       <bigram_embedder.predicto<...>object at 0x7f79fef2b240>\n",
      "random                    module                        <module 'random' from '/h<...>lib/python3.6/random.py'>\n",
      "random_n_ids              list                          n=10\n",
      "sub                       builtin_function_or_method    <built-in function sub>\n",
      "test_zip                  list                          n=2500\n",
      "torch                     module                        <module 'torch' from '/ho<...>kages/torch/__init__.py'>\n",
      "vec_hat                   list                          n=100\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/nearest_vecs_test' + SUFFIX + '.pkl', 'wb') as f:\n",
    "#     pickle.dump(nearest_vecs_test, f)\n",
    "    \n",
    "# with open('data/nearest_ids_test' + SUFFIX + '.pkl', 'wb') as f:\n",
    "#     pickle.dump(nearest_ids_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_example(w1, w2, N=5):\n",
    "    vec1 = all_vecs[all_bigrams.index(w1)]\n",
    "    vec2 = all_vecs[all_bigrams.index(w2)]\n",
    "    x = {'w1_vec_str': ' '.join([str(v) for v in vec1]), 'w2_vec_str': ' '.join([str(v) for v in vec2])}\n",
    "    bi_vec = predictor.predict_json(x)['bigram_vecs_hat']\n",
    "    _, ids_hat = get_closest(bi_vec, all_vecs, N)\n",
    "    bigrams_hat = [all_bigrams[i] for i in ids_hat]\n",
    "    return bigrams_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['another_album',\n",
       " 'album_another',\n",
       " 'following_album',\n",
       " 'album_itself',\n",
       " 'this_album']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'this album'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modern_methods',\n",
       " 'traditional_methods',\n",
       " 'conventional_methods',\n",
       " 'historical_method',\n",
       " 'modern_techniques']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'contemporary methods'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rephrase', 'you_something', 'damn_thing', 'you_simply', 'says_you']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'rephrase me'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['say_something',\n",
       " 'know_something',\n",
       " 'that_something',\n",
       " 'us_something',\n",
       " 'says_something']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'say something'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'three_domain', 'appropriate_name', 'name_now', 'temporary_name']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'domain name'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dirty_bastard', 'crazy_little', 'screaming_in', 'stray_dog', 'pretty_hate']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'thug live'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trance_state',\n",
       " 'the_art',\n",
       " 'great_state',\n",
       " 'state_history',\n",
       " 'general_understanding']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'state_of the_art'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trance_state',\n",
       " 'art',\n",
       " 'western_art',\n",
       " 'contemporary_culture',\n",
       " 'intellectual_history']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'state_of art'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of_art',\n",
       " 'western_art',\n",
       " 'national_art',\n",
       " 'cultural_institutions',\n",
       " 'that_institution']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'state of_art'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substract_add(n1, n2, n3, N=5):\n",
    "    try:\n",
    "        vec1 = np.array(all_vecs[all_bigrams.index(n1)])\n",
    "    except(ValueError):\n",
    "        print('OOV ngram; estimating vec for it')\n",
    "        vec1 = pred_example(n1.split('_')[0], n1.split('_')[1], return_vec=True)\n",
    "        \n",
    "    vec2 = np.array(all_vecs[all_bigrams.index(n2)])\n",
    "    vec3 = np.array(all_vecs[all_bigrams.index(n3)])\n",
    "    #x = {'w1_vec_str': ' '.join([str(v) for v in vec1]), 'w2_vec_str': ' '.join([str(v) for v in vec2])}\n",
    "    v_res = vec1 - vec2 + vec3\n",
    "    # bi_vec = predictor.predict_json(x)['bigram_vecs_hat']\n",
    "    _, ids_hat = get_closest(v_res, all_vecs, N)\n",
    "    bigrams_hat = [all_bigrams[i] for i in ids_hat]\n",
    "    return bigrams_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'running_for', 'also_running', 'that_running', 'at_running']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substract_add('go_to', 'go', 'run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennmt]",
   "language": "python",
   "name": "conda-env-allennmt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
