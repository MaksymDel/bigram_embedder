{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Extract bigram vectors we will train and eval a model on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars\n",
    "PATH_DATA_STEP3_FOLDER = \"../data/step3_fasttext_vectors/\"\n",
    "PATH_FASTTEXT_VECTORS = PATH_DATA_STEP3_FOLDER + \"vectors_likelihood_ratio-300-0.05.vec\"\n",
    "SUFFIX = PATH_FASTTEXT_VECTORS.split('vectors_')[1][0:-4]\n",
    "PATH_DATA_OUTPUT_FOLDER = \"../data/step4_wordpairs_bigrams_vec_data/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fasttext .vec file containing unigrams and bigrams\n",
    "with open(PATH_FASTTEXT_VECTORS, 'r') as f:\n",
    "    vectors_all = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vectors_all, dim = vectors_all[0].split()\n",
    "num_vectors_all, dim = int(num_vectors_all), int(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808968\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(num_vectors_all)\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectors_all[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract word pair - bigram vector pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_types_all = []\n",
    "ngram_vectors_all = []\n",
    "for l in vectors_all:\n",
    "    l = l.split()\n",
    "    ngram_types_all.append(l[0])\n",
    "    ngram_vectors_all.append([float(v) for v in l[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'one', 'and', 'zero', 'in', 'of_the', 'two', 'a', 'one_nine']\n"
     ]
    }
   ],
   "source": [
    "print(ngram_types_all[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_type2vec = dict(zip(ngram_types_all, ngram_vectors_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_types = [ngram for ngram in ngram_types_all if '_' in ngram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we got 590967 bigrams & 218001 unigrams\n"
     ]
    }
   ],
   "source": [
    "print('we got', len(bigram_types), 'bigrams &', len(ngram_types_all) - len(bigram_types), 'unigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct parallel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpair_vecs = []\n",
    "bigram_vecs = []\n",
    "bigrams_choosen = []\n",
    "for bigram in bigram_types:\n",
    "    try:\n",
    "        word1, word2 = bigram.split('_')\n",
    "        word1_vec, word2_vec = ngram_type2vec[word1], ngram_type2vec[word2]\n",
    "\n",
    "        bigram_vec = ngram_type2vec[bigram]\n",
    "\n",
    "        wordpair_vecs.append((word1_vec, word2_vec))\n",
    "        bigram_vecs.append(bigram_vec)\n",
    "        bigrams_choosen.append(bigram)\n",
    "    except(KeyError): # occures when one of bigram words is not in vocabulary\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590558\n",
      "590558\n",
      "590558\n"
     ]
    }
   ],
   "source": [
    "print(len(bigram_vecs))\n",
    "print(len(wordpair_vecs))\n",
    "print(len(bigrams_choosen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_DATA_OUTPUT_FOLDER + 'data_bigram_vecs_' + SUFFIX + '.pkl', 'wb') as f:\n",
    "    pickle.dump(bigram_vecs, f)\n",
    "\n",
    "with open(PATH_DATA_OUTPUT_FOLDER + 'data_wordpair_vecs_' + SUFFIX + '.pkl', 'wb') as f:\n",
    "    pickle.dump(wordpair_vecs, f)\n",
    "    \n",
    "with open(PATH_DATA_OUTPUT_FOLDER + 'data_bigram_types_' + SUFFIX + '.pkl', 'wb') as f:\n",
    "    pickle.dump(bigrams_choosen, f)\n",
    "\n",
    "with open(PATH_DATA_OUTPUT_FOLDER + 'data_bigram_types_' + SUFFIX + '.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(bigrams_choosen))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennmt]",
   "language": "python",
   "name": "conda-env-allennmt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
