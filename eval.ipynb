{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions and get results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/del/anaconda3/envs/allennmt/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from allennlp.models.archival import archive_model\n",
    "import pickle\n",
    "import numpy as np\n",
    "#archive_model(\"models/bigram_embedder_feedforw_l1_tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.service.predictors import Predictor\n",
    "\n",
    "# required so that our custom model + predictor + dataset reader\n",
    "# will be registered by name\n",
    "import bigram_embedder\n",
    "\n",
    "# with open(\"data/step5_holduot_splits/train_all_shuffled.pkl\", 'rb') as f:\n",
    "#     train_zip = pickle.load(f)\n",
    "# with open(\"data/step5_holduot_splits/dev_all_shuffled.pkl\", 'rb') as f:\n",
    "#     dev_zip = pickle.load(f)\n",
    "with open(\"data/step5_holduot_splits/test_all_shuffled.pkl\", 'rb') as f:\n",
    "    test_zip = pickle.load(f)\n",
    "    \n",
    "\n",
    "archive = load_archive('models/bigram_embedder_feedforw_l1_tanh/model.tar.gz')\n",
    "predictor = Predictor.from_archive(archive, 'bigram-embedder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/step5_holduot_splits/test_all_shuffled.tsv\") as f:\n",
    "    X_test = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_types_test = []\n",
    "bigram_vecs_test = []\n",
    "for l in X_test:\n",
    "    bt, bv, _, _ = l.split('\\t')\n",
    "    bigram_types_test.append(bt)\n",
    "    bigram_vecs_test.append([float(v) for v in bv.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vecs_test_hat = []\n",
    "for ex in X_test:\n",
    "    ex = predictor.load_line(ex)\n",
    "    bigram_vecs_test_hat.append(predictor.predict_json(ex).get(\"bigram_vecs_hat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_smooth_l1_error(y, y_hat):\n",
    "    y_var = torch.autograd.Variable(torch.FloatTensor(y))\n",
    "    y_hat_var = torch.autograd.Variable(torch.FloatTensor(y_hat))\n",
    "    \n",
    "    if y_var.size() != y_hat_var.size():\n",
    "        smooth_l1_error = torch.nn.SmoothL1Loss(reduce=False)\n",
    "        y_var = y_var.expand(y_hat_var.size())\n",
    "        return smooth_l1_error.forward(y_var, y_hat_var).sum(1).data\n",
    "\n",
    "    else:\n",
    "        smooth_l1_error = torch.nn.SmoothL1Loss()\n",
    "        return smooth_l1_error.forward(y_var, y_hat_var).data[0]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comput accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(target, vecs, n):\n",
    "    sim_matrix = compute_smooth_l1_error(target, vecs)\n",
    "    nvecs, nids = sim_matrix.sort(0)\n",
    "    return nvecs[0:n], nids[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars\n",
    "PATH_DATA_STEP3_FOLDER = \"data/step3_fasttext_vectors/\"\n",
    "PATH_FASTTEXT_VECTORS = PATH_DATA_STEP3_FOLDER + \"vectors_likelihood_ratio-100-0.05.vec\"\n",
    "SUFFIX = PATH_FASTTEXT_VECTORS.split('vectors_')[1][0:-4]\n",
    "PATH_DATA_OUTPUT_FOLDER = \"data/step4_wordpairs_bigrams_vec_data/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_ngram_vecs():\n",
    "    # load fasttext .vec file containing unigrams and bigrams\n",
    "    with open(PATH_FASTTEXT_VECTORS, 'r') as f:\n",
    "        vectors_all = f.readlines()\n",
    "        \n",
    "    num_vectors_all, dim = vectors_all[0].split()\n",
    "    num_vectors_all, dim = int(num_vectors_all), int(dim)\n",
    "    \n",
    "    print(num_vectors_all)\n",
    "    print(dim)\n",
    "    \n",
    "    del vectors_all[0]\n",
    "    \n",
    "    ngram_types_all = []\n",
    "    ngram_vectors_all = []\n",
    "    for l in vectors_all:\n",
    "        l = l.split()\n",
    "        ngram_types_all.append(l[0])\n",
    "        ngram_vectors_all.append([float(v) for v in l[1:]])\n",
    "    return ngram_types_all, ngram_vectors_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808968\n",
      "100\n",
      "CPU times: user 31 s, sys: 1.63 s, total: 32.6 s\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#all_bigrams, all_vecs, _ = zip(*train_zip + dev_zip + test_zip)\n",
    "all_bigrams, all_vecs = get_all_ngram_vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "CPU times: user 46min 51s, sys: 8min 1s, total: 54min 53s\n",
      "Wall time: 54min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUM_NEIGHBOURS = 100\n",
    "nearest_vecs_test = []\n",
    "nearest_ids_test = []\n",
    "for i, vec_hat in enumerate(bigram_vecs_test_hat):\n",
    "    nvecs, nids = get_closest(vec_hat, all_vecs, NUM_NEIGHBOURS)\n",
    "    nearest_vecs_test.append(nvecs)\n",
    "    nearest_ids_test.append(nids)\n",
    "    if i % 100 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_bigrams = []\n",
    "for i, ids in enumerate(nearest_ids_test):\n",
    "    predicted_bigram_list = [all_bigrams[i] for i in ids]\n",
    "    predicted_bigrams.append(predicted_bigram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 accuaracy on test set: 0.7836\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "corrects = []\n",
    "corrects_groundtruth = []\n",
    "errors = []\n",
    "errors_groundtruth = [] \n",
    "for i, predicted_bigrams_list in enumerate(predicted_bigrams):\n",
    "    if bigram_types_test[i] in predicted_bigrams_list[0:N]:\n",
    "        corrects.append(predicted_bigrams_list)\n",
    "        corrects_groundtruth.append(bigram_types_test[i])\n",
    "    else:\n",
    "        errors.append(predicted_bigrams_list)\n",
    "        errors_groundtruth.append(bigram_types_test[i])\n",
    "        \n",
    "print('Top', N, 'accuaracy on test set:', len(corrects) / len(predicted_bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318 | the_aqua -> ['the_ole', 'the_ea', 'the_pandemonium', 'the_brat', 'the_splash']\n",
      "298 | page_don -> ['page_and', 'page_said', 'whatlinkshere', 'taylor_page', 'page']\n",
      "66 | now_made -> ['now', 'now_making', 'previously_made', 'already_included', 'made_also']\n",
      "255 | january_is -> ['december_is', 'february_is', 'november_is', 'september_is', 'october_is']\n",
      "432 | of_accepted -> ['immediately_accepted', 'finally_accepted', 'following_accepted', 'which_accepted', 'accepted_with']\n",
      "402 | party_six -> ['party_three', 'party_seven', 'party_five', 'three_party', 'party_two']\n",
      "154 | by_mikhail -> ['and_leonid', 'chervenkov', 'prutkov', 'vladimir_and', 'morozevich']\n",
      "201 | kb_to -> ['four_kib', 'seven_kb', 'mib_of', 'kb_and', 'mb_and']\n",
      "152 | produced_when -> ['eventually_produced', 'produced_before', 'produced_that', 'produced_although', 'produced_after']\n",
      "459 | supplies_a -> ['supplying_a', 'supplying_the', 'which_supplies', 'the_supplies', 'supplies_the']\n"
     ]
    }
   ],
   "source": [
    "random_n_ids = [random.randrange(0, len(errors)) for i in range(n)]\n",
    "\n",
    "for ind in random_n_ids:\n",
    "    ngrams = errors[ind]\n",
    "    print(ind, '|', errors_groundtruth[ind], '->', ngrams[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-65e83cba8886>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/nearest_vecs_test'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSUFFIX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnearest_vecs_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/nearest_ids_test'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSUFFIX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnearest_ids_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with open('data/nearest_vecs_test' + SUFFIX + '.pkl', 'wb') as f:\n",
    "#     pickle.dump(nearest_vecs_test, f)\n",
    "    \n",
    "# with open('data/nearest_ids_test' + SUFFIX + '.pkl', 'wb') as f:\n",
    "#     pickle.dump(nearest_ids_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_example(w1, w2, N=5, return_vec=False):\n",
    "    vec1 = all_vecs[all_bigrams.index(w1)]\n",
    "    vec2 = all_vecs[all_bigrams.index(w2)]\n",
    "    x = {'w1_vec_str': ' '.join([str(v) for v in vec1]), 'w2_vec_str': ' '.join([str(v) for v in vec2])}\n",
    "    bi_vec = predictor.predict_json(x)['bigram_vecs_hat']\n",
    "    if return_vec:\n",
    "        return bi_vec\n",
    "    _, ids_hat = get_closest(bi_vec, all_vecs, N)\n",
    "    bigrams_hat = [all_bigrams[i] for i in ids_hat]\n",
    "    return bigrams_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modern_methods',\n",
       " 'traditional_methods',\n",
       " 'conventional_methods',\n",
       " 'historical_method',\n",
       " 'modern_techniques']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'contemporary methods'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rephrase', 'you_something', 'damn_thing', 'says_something', 'say_something']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'rephrase me'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['say_something',\n",
       " 'know_something',\n",
       " 'us_something',\n",
       " 'that_something',\n",
       " 'says_something']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'say something'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'three_domain', 'temporary_name', 'appropriate_name', 'name_now']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'domain name'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thug', 'dirty_bastard', 'dumb_blonde', 'marty_and', 'little_guy']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'thug guy'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trance_state', 'the_art', 'great_state', 'state_history', 'state_historical']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'state_of the_art'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trance_state',\n",
       " 'art',\n",
       " 'western_art',\n",
       " 'intellectual_history',\n",
       " 'contemporary_culture']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'state_of art'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['western_art',\n",
       " 'cultural_institutions',\n",
       " 'that_institution',\n",
       " 'national_art',\n",
       " 'institution_from']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = 'state of_art'\n",
    "w1, w2 = pair.split(' ')\n",
    "pred_example(w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Ngram Arithmetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substract_add(n1, n2, n3, N=5):\n",
    "    try:\n",
    "        vec1 = np.array(all_vecs[all_bigrams.index(n1)])\n",
    "    except(ValueError):\n",
    "        print('OOV ngram; estimating vec for it')\n",
    "        vec1 = pred_example(n1.split('_')[0], n1.split('_')[1], return_vec=True)\n",
    "        \n",
    "    vec2 = np.array(all_vecs[all_bigrams.index(n2)])\n",
    "    vec3 = np.array(all_vecs[all_bigrams.index(n3)])\n",
    "    #x = {'w1_vec_str': ' '.join([str(v) for v in vec1]), 'w2_vec_str': ' '.join([str(v) for v in vec2])}\n",
    "    v_res = vec1 - vec2 + vec3\n",
    "    # bi_vec = predictor.predict_json(x)['bigram_vecs_hat']\n",
    "    _, ids_hat = get_closest(v_res, all_vecs, N)\n",
    "    bigrams_hat = [all_bigrams[i] for i in ids_hat]\n",
    "    return bigrams_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV ngram; estimating vec for it\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['easy',\n",
       " 'quick_way',\n",
       " 'good_job',\n",
       " 'easy_way',\n",
       " 'getting_ready',\n",
       " 'extra_effort',\n",
       " 'little_need',\n",
       " 'easygoing',\n",
       " 'work_just',\n",
       " 'lot_easier']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substract_add('hard_job', 'hard', 'easy', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hate_you',\n",
       " 'hate',\n",
       " 'hate_me',\n",
       " 'fuckin',\n",
       " 'i_hate',\n",
       " 'hate_it',\n",
       " 'wrong_you',\n",
       " 'me_feel',\n",
       " 'me_wrong',\n",
       " 'you_something']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substract_add('like_it', 'like', 'hate', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['me',\n",
       " 'like_me',\n",
       " 'like_you',\n",
       " 'me_now',\n",
       " 'me_and',\n",
       " 'like_i',\n",
       " 'me_like',\n",
       " 'brant_me',\n",
       " 'me_away',\n",
       " 'and_me']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substract_add('like_you', 'you', 'me', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['guitar_music',\n",
       " 'rock_guitar',\n",
       " 'guitar',\n",
       " 's_guitar',\n",
       " 'guitar_acoustic',\n",
       " 'guitar_producer',\n",
       " 'guitar_dave',\n",
       " 'guitar_guitar',\n",
       " 'guitar_steve',\n",
       " 'guitar_percussion']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substract_add('rap_music', 'rhymes', 'guitar', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rhymes',\n",
       " 'rhymes_and',\n",
       " 'rhymes_in',\n",
       " 'it_rhymes',\n",
       " 'pop_cultural',\n",
       " 'punk_culture',\n",
       " 'hip_hop',\n",
       " 'alcopop',\n",
       " 'hop_culture',\n",
       " 'rhyme']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substract_add('rock_music', 'guitar', 'rhymes', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country_australia',\n",
       " 'kingdom_london',\n",
       " 'london_most',\n",
       " 'london_all',\n",
       " 'countrylink',\n",
       " 'country',\n",
       " 'london_also',\n",
       " 'london_region',\n",
       " 'london_free',\n",
       " 'england_international']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substract_add('london_city', 'city', 'country', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city_paris',\n",
       " 'paris',\n",
       " 'paris_paris',\n",
       " 'paris_metro',\n",
       " 'paris_commune',\n",
       " 'paris_l',\n",
       " 'paris_la',\n",
       " 'paris_dakar',\n",
       " 'ra_paris',\n",
       " 'paris_new']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substract_add('london_city', 'london', 'paris', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paris_metro',\n",
       " 'city_paris',\n",
       " 'paris_m',\n",
       " 'paris_commune',\n",
       " 'paris_cdg',\n",
       " 'paris_l',\n",
       " 'centrale_paris',\n",
       " 'paris',\n",
       " 'paris_dakar',\n",
       " 'paris_lyon']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substract_add('london_city', 'england', 'france', 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennmt]",
   "language": "python",
   "name": "conda-env-allennmt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
